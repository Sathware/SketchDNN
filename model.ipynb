{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "from typing import Tuple\n",
    "from typing import Any\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from config import NODE_FEATURE_DIMENSION, EDGE_FEATURE_DIMENSION, MAX_NUM_PRIMITIVES, HYPER_PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GVAE(nn.Module):\n",
    "    def __init__(self, device : torch.device):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.graph_emb_dim = 512\n",
    "        self.node_dim = NODE_FEATURE_DIMENSION\n",
    "        self.edge_dim = EDGE_FEATURE_DIMENSION\n",
    "\n",
    "        self.encoder_num_tf_layers = 4\n",
    "        self.encoder_num_attention_heads = 8\n",
    "\n",
    "        self.decoder_num_tf_layers = 4\n",
    "        self.decoder_num_attention_heads = 8\n",
    "\n",
    "        self.encoder = TransformerEncoder(node_dim = self.node_dim, \n",
    "                                          edge_dim = self.edge_dim, \n",
    "                                          graph_emb_dim = self.graph_emb_dim, \n",
    "                                          num_tf_layers = self.encoder_num_tf_layers, \n",
    "                                          num_heads = self.encoder_num_attention_heads,\n",
    "                                          device = self.device\n",
    "                                         )\n",
    "        \n",
    "        self.decoder = TransformerDecoder(node_dim = self.node_dim, \n",
    "                                          edge_dim = self.edge_dim, \n",
    "                                          graph_emb_dim = self.graph_emb_dim,\n",
    "                                          num_tf_layers = self.decoder_num_tf_layers,\n",
    "                                          num_heads = self.decoder_num_attention_heads, \n",
    "                                          device = self.device\n",
    "                                         )\n",
    "    \n",
    "    def forward(self, nodes : Tensor, edges : Tensor):\n",
    "        means, logvars = self.encoder(nodes, edges)\n",
    "        latents = self.sample_latent(means, torch.exp(0.5 * logvars))\n",
    "        pred_nodes, pred_edges = self.decoder(latents)\n",
    "\n",
    "        return pred_nodes, pred_edges, means, logvars\n",
    "    \n",
    "    def sample_latent(self, mean : Tensor, standard_deviation : Tensor):\n",
    "        epsilon = torch.randn(size=mean.size(), device = self.device)\n",
    "        latents = mean + standard_deviation * epsilon\n",
    "        return latents\n",
    "    \n",
    "    def sample_graph(self) -> Tuple[Tensor, Tensor]:\n",
    "        latent = torch.randn(size = (self.graph_emb_dim,), device = self.device)\n",
    "        nodes, edges = self.decoder(latent)\n",
    "        nodes = nodes.detach()\n",
    "        edges = edges.detach()\n",
    "\n",
    "        nodes[:,:,1:6] = torch.exp(nodes[:,:,1:6], dim = 2) # Softmax for primitive classes\n",
    "        \n",
    "        # softmax for constraints; Conceptual map => n1 (out) -> n2 (in) i.e. out_node, edge, in_node\n",
    "        edges[:,:,:,0:4] = torch.exp(edges[:,:,:,0:4], dim = 3) # Softmax for out_node subnode type\n",
    "        edges[:,:,:,4:8] = torch.exp(edges[:,:,:,4:8], dim = 3) # Softmax for in_node subnode type\n",
    "        edges[:,:,:,8: ] = torch.exp(edges[:,:,:,8: ], dim = 3) # Softmax for edge type\n",
    "        return torch.squeeze(nodes), torch.squeeze(edges)\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, node_dim : int, edge_dim : int, graph_emb_dim : int, num_tf_layers: int, num_heads : int, device : torch.device): # perm_emb_dim: int,\n",
    "        super().__init__()\n",
    "        self.node_dim = node_dim # Number of features per node\n",
    "        self.edge_dim = edge_dim # Number of features per edge\n",
    "        self.graph_emb_dim = graph_emb_dim # Size of graph embedding vector\n",
    "        # self.perm_emb_dim = perm_emb_dim # Size of permutation embedding vector\n",
    "\n",
    "        self.mlp_node_hidden_dim = 64\n",
    "        self.mlp_edge_hidden_dim = 32\n",
    "\n",
    "        self.tf_node_hidden_dim = 32\n",
    "        self.tf_edge_hidden_dim = 16\n",
    "\n",
    "        self.num_tf_layers = num_tf_layers\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.mlp_in_nodes = nn.Sequential(nn.Linear(in_features = self.node_dim, out_features = self.mlp_node_hidden_dim, device = device),\n",
    "                                          nn.LeakyReLU(),\n",
    "                                          nn.Linear(in_features = self.mlp_node_hidden_dim, out_features = self.tf_node_hidden_dim, device = device),\n",
    "                                          nn.LeakyReLU()\n",
    "                                         )\n",
    "\n",
    "        self.mlp_in_edges = nn.Sequential(nn.Linear(in_features = self.edge_dim, out_features = self.mlp_edge_hidden_dim, device = device),\n",
    "                                          nn.LeakyReLU(),\n",
    "                                          nn.Linear(in_features = self.mlp_edge_hidden_dim, out_features = self.tf_edge_hidden_dim, device = device),\n",
    "                                          nn.LeakyReLU()\n",
    "                                         )\n",
    "        \n",
    "        self.tf_layers = nn.ModuleList([TransformerLayer(num_heads = self.num_heads, \n",
    "                                                         in_node_dim = self.tf_node_hidden_dim, \n",
    "                                                         out_node_dim = self.tf_node_hidden_dim,\n",
    "                                                         in_edge_dim = self.tf_edge_hidden_dim,\n",
    "                                                         out_edge_dim = self.tf_edge_hidden_dim, \n",
    "                                                         device = device\n",
    "                                                        ) \n",
    "                                        for _ in range(self.num_tf_layers)])\n",
    "        \n",
    "        self.lin_edge = nn.Linear(in_features = self.tf_edge_hidden_dim, out_features = self.tf_node_hidden_dim, device = device)\n",
    "        self.pna = PNA()\n",
    "\n",
    "        self.mlp_mean = nn.Sequential(nn.Linear(in_features = 4 * self.tf_node_hidden_dim, out_features = self.tf_node_hidden_dim, device = device),\n",
    "                                      nn.LeakyReLU(),\n",
    "                                      nn.Linear(in_features = self.tf_node_hidden_dim, out_features = self.graph_emb_dim, device = device),\n",
    "                                      nn.LeakyReLU()\n",
    "                                     )\n",
    "        \n",
    "        self.mlp_logvar = nn.Sequential(nn.Linear(in_features = 4 * self.tf_node_hidden_dim, out_features = self.tf_node_hidden_dim, device = device),\n",
    "                                        nn.LeakyReLU(),\n",
    "                                        nn.Linear(in_features = self.tf_node_hidden_dim, out_features = self.graph_emb_dim, device = device),\n",
    "                                        nn.LeakyReLU()\n",
    "                                       )\n",
    "\n",
    "    def forward(self, nodes : Tensor, edges : Tensor):\n",
    "        nodes = self.mlp_in_nodes(nodes) # batch_size x num_nodes x mlp_node_hidden_dim\n",
    "        edges = self.mlp_in_edges(edges) # batch_size x num_nodes x num_nodes x mlp_edge_hidden_dim\n",
    "\n",
    "        for layer in self.tf_layers:\n",
    "            nodes, edges = layer(nodes, edges) # batch_size x num_nodes x tf_node_hidden_dim ; batch_size x num_nodes x num_nodes x tf_edge_hidden_dim\n",
    "        \n",
    "        edges = self.lin_edge(edges) # batch_size x num_nodes x num_nodes x tf_node_hidden_dim\n",
    "\n",
    "        nodes = self.pna(nodes, dim = (1))   # batch_size x (4 * tf_node_hidden_dim)\n",
    "        edges = self.pna(edges, dim = (1,2)) # batch_size x (4 * tf_node_hidden_dim)\n",
    "\n",
    "        graph_embs = nodes + edges # batch_size x (4 * tf_node_hidden_dim)\n",
    "\n",
    "        means = self.mlp_mean(graph_embs)     # batch_size x graph_emb_dim\n",
    "        logvars = self.mlp_logvar(graph_embs) # batch_size x graph_emb_dim\n",
    "\n",
    "        return means, logvars     \n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self, node_dim : int, edge_dim : int, graph_emb_dim : int, num_tf_layers : int, num_heads : int, device : torch.device):\n",
    "        super().__init__()\n",
    "        self.graph_emb_dim = graph_emb_dim\n",
    "\n",
    "        self.mlp_in_node_dim = 32\n",
    "        self.mlp_out_node_dim = 64\n",
    "        self.mlp_in_edge_dim = 24\n",
    "        self.mlp_out_edge_dim = 32\n",
    "\n",
    "        self.tf_node_hidden_dim = 64\n",
    "        self.tf_edge_hidden_dim = 32\n",
    "        self.num_tf_layers = num_tf_layers\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.mlp_create_nodes = nn.Sequential(nn.Linear(in_features = self.graph_emb_dim, out_features = MAX_NUM_PRIMITIVES * self.mlp_in_node_dim, device = device),\n",
    "                                              nn.LeakyReLU(),\n",
    "                                              nn.Linear(in_features = MAX_NUM_PRIMITIVES * self.mlp_in_node_dim, out_features = MAX_NUM_PRIMITIVES * self.mlp_out_node_dim, device = device),\n",
    "                                              nn.LeakyReLU()\n",
    "                                             )\n",
    "        \n",
    "        self.mlp_create_edges = nn.Sequential(nn.Linear(in_features = self.graph_emb_dim, \n",
    "                                                        out_features = MAX_NUM_PRIMITIVES * MAX_NUM_PRIMITIVES * self.mlp_in_edge_dim, \n",
    "                                                        device = device\n",
    "                                                       ),\n",
    "                                              nn.LeakyReLU(),\n",
    "                                              nn.Linear(in_features = MAX_NUM_PRIMITIVES * MAX_NUM_PRIMITIVES * self.mlp_in_edge_dim, \n",
    "                                                        out_features = MAX_NUM_PRIMITIVES * MAX_NUM_PRIMITIVES * self.mlp_out_edge_dim, \n",
    "                                                        device = device\n",
    "                                                       ),\n",
    "                                              nn.LeakyReLU()\n",
    "                                             )\n",
    "\n",
    "        self.tf_layers = nn.ModuleList([TransformerLayer(num_heads = self.num_heads, \n",
    "                                                         in_node_dim = self.tf_node_hidden_dim, \n",
    "                                                         out_node_dim = self.tf_node_hidden_dim,\n",
    "                                                         in_edge_dim = self.tf_edge_hidden_dim,\n",
    "                                                         out_edge_dim = self.tf_edge_hidden_dim, \n",
    "                                                         device = device\n",
    "                                                        ) \n",
    "                                        for _ in range(self.num_tf_layers)])\n",
    "\n",
    "        self.mlp_out_nodes = nn.Sequential(nn.Linear(in_features = self.mlp_out_node_dim, out_features = self.mlp_out_node_dim, device = device),\n",
    "                                           nn.LeakyReLU(),\n",
    "                                           nn.Linear(in_features = self.mlp_out_node_dim, out_features = node_dim, device = device)\n",
    "                                          )\n",
    "\n",
    "        self.mlp_out_edges = nn.Sequential(nn.Linear(in_features = self.mlp_out_edge_dim, out_features = self.mlp_out_edge_dim, device = device),\n",
    "                                           nn.LeakyReLU(),\n",
    "                                           nn.Linear(in_features = self.mlp_out_edge_dim, out_features = edge_dim, device = device)\n",
    "                                          )\n",
    "\n",
    "    def forward(self, latents):\n",
    "        temp_nodes = self.mlp_create_nodes(latents)\n",
    "        nodes = torch.reshape(input = temp_nodes, shape = (-1, MAX_NUM_PRIMITIVES, self.mlp_out_node_dim))                     # batch_size x num_nodes x mlp_node_out_dim\n",
    "        edges = torch.reshape(input = self.mlp_create_edges(latents), shape = (-1, MAX_NUM_PRIMITIVES, MAX_NUM_PRIMITIVES, self.mlp_out_edge_dim)) # batch_size x num_nodes x num_nodes x mlp_edge_out_dim\n",
    "\n",
    "        for layer in self.tf_layers:\n",
    "            nodes, edges = layer(nodes, edges) # batch_size x num_nodes x tf_node_hidden_dim ; # batch_size x num_nodes x num_nodes x tf_edge_hidden_dim\n",
    "        \n",
    "        nodes = self.mlp_out_nodes(nodes) # batch_size x num_nodes x node_dim\n",
    "        edges = self.mlp_out_edges(edges) # batch_size x num_nodes x num_nodes x edge_dim\n",
    "\n",
    "        # sigmoid and logsoftmax for nodes\n",
    "        nodes[:,:,0] = F.sigmoid(nodes[:,:,0])                  # Sigmoid for isConstructible\n",
    "        nodes[:,:,1:6] = F.log_softmax(nodes[:,:,1:6], dim = 2) # LogSoftmax for primitive classes (i.e. line, circle, arc, point, none)\n",
    "        \n",
    "        # logsoftmax for constraints; Conceptual map => n1 (out) -> n2 (in) i.e. out_node, edge, in_node\n",
    "        edges[:,:,:,0:4] = F.log_softmax(edges[:,:,:,0:4], dim = 3) # Softmax for out_node subnode type\n",
    "        edges[:,:,:,4:8] = F.log_softmax(edges[:,:,:,4:8], dim = 3) # Softmax for in_node subnode type\n",
    "        edges[:,:,:,8: ] = F.log_softmax(edges[:,:,:,8: ], dim = 3) # Softmax for edge type\n",
    "\n",
    "        return nodes, edges\n",
    "\n",
    "# Graph Transformer Layer outlined by DiGress Graph Diffusion\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, num_heads : int, in_node_dim : int, out_node_dim : int, in_edge_dim : int, out_edge_dim : int, device : torch.device):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.in_node_dim = in_node_dim\n",
    "        self.out_node_dim = out_node_dim\n",
    "        self.in_edge_dim = in_edge_dim\n",
    "        self.out_edge_dim = out_edge_dim\n",
    "\n",
    "        self.attention_heads = nn.ModuleList([AttentionHead(in_node_dim = self.in_node_dim, \n",
    "                                                            out_node_dim = self.out_node_dim, \n",
    "                                                            in_edge_dim = self.in_edge_dim, \n",
    "                                                            device = device\n",
    "                                                           )\n",
    "                                              for _ in range(self.num_heads)])\n",
    "        \n",
    "        self.lin_nodes = nn.Linear(in_features = self.num_heads * self.out_node_dim, out_features = self.out_node_dim, device = device)\n",
    "        self.lin_edges = nn.Linear(in_features = self.num_heads * self.out_node_dim, out_features = self.out_edge_dim, device = device)\n",
    "\n",
    "        # self.mlp_dropout_nodes = nn.Sequential(nn.Linear(in_features = self.num_heads * self.out_node_dim, out_features = self.out_node_dim, device = device),\n",
    "        #                                        nn.LeakyReLU(),\n",
    "        #                                        nn.Linear(in_features = self.out_node_dim, out_features = self.out_node_dim, device = device),\n",
    "        #                                        nn.LeakyReLU()\n",
    "        #                                       )\n",
    "        \n",
    "        # self.mlp_dropout_edges = nn.Sequential(nn.Linear(in_features = self.num_heads * self.out_node_dim, out_features = self.out_edge_dim, device = device),\n",
    "        #                                        nn.LeakyReLU(),\n",
    "        #                                        nn.Linear(in_features = self.out_edge_dim, out_features = self.out_edge_dim, device = device),\n",
    "        #                                        nn.LeakyReLU()\n",
    "        #                                       )\n",
    "        # self.lnorm2_edges = nn.LayerNorm(normalized_shape = self.out_edge_dim, device = device)\n",
    "        \n",
    "    \n",
    "    def forward(self, nodes : Tensor, edges : Tensor) -> Tuple[Tensor, Tensor]:\n",
    "        # Collect the output of all attention heads ; *Note nodes and edges are NOT on different devices\n",
    "        nodes_heads = torch.Tensor().to(nodes.device)\n",
    "        edges_heads = torch.Tensor().to(edges.device)\n",
    "        for attention_head in self.attention_heads:\n",
    "            temp_nodes, temp_edges = attention_head(nodes, edges) # nodes = batch_size x num_nodes x out_node_dim; edges = batch_size x num_nodes x num_nodes x out_node_dim\n",
    "            nodes_heads = torch.cat(tensors = (nodes_heads, temp_nodes), dim = 2)\n",
    "            edges_heads = torch.cat(tensors = (edges_heads, temp_edges), dim = 3)\n",
    "        # now nodes_heads = batch_size x num_nodes x (num_heads * out_node_dim)\n",
    "        # now edges_heads = batch_size x num_nodes x num_nodes x (num_heads * out_node_dim)\n",
    "\n",
    "        # Combine Attention Head Output\n",
    "        new_nodes = self.lin_nodes(nodes_heads) # batch_size x num_nodes x out_node_dim\n",
    "        new_edges = self.lin_edges(edges_heads) # batch_size x num_nodes x num_nodes x out_edge_dim\n",
    "\n",
    "        # Dropout & Layer Norm\n",
    "        #temp_new_nodes = self.mlp_dropout_nodes(new_nodes)\n",
    "        #temp_new_edges = self.mlp_dropout_edges(new_edges)\n",
    "\n",
    "        #new_nodes = self.lnorm1_nodes(temp_new_nodes + new_nodes)\n",
    "        #new_edges = self.lnorm1_edges(temp_new_edges + new_edges)\n",
    "\n",
    "        # Transform into output nodes and edges\n",
    "        #temp_new_nodes = self.mlp_dropout_nodes(new_nodes) # batch_size x num_nodes x out_node_dim\n",
    "        #temp_new_edges = self.mlp_dropout_edges(new_edges) # batch_size x num_nodes x num_nodes x out_edge_dim\n",
    "\n",
    "        # Second Layer Norm\n",
    "        #new_nodes = self.lnorm2_nodes(temp_new_nodes + new_nodes)\n",
    "        #new_edges = self.lnorm2_edges(temp_new_edges + new_edges)\n",
    "\n",
    "        return new_nodes, new_edges\n",
    "\n",
    "# Outer Product Attention Head\n",
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, in_node_dim : int, out_node_dim : int, in_edge_dim : int, device : torch.device):\n",
    "        super().__init__()\n",
    "        self.in_node_dim = in_node_dim\n",
    "        self.out_node_dim = out_node_dim\n",
    "        self.in_edge_dim = in_edge_dim\n",
    "\n",
    "        self.lin_query = nn.Linear(in_features = self.in_node_dim, out_features = self.out_node_dim, device = device)\n",
    "        self.lin_key = nn.Linear(in_features = self.in_node_dim, out_features = self.out_node_dim, device = device)\n",
    "        self.lin_value = nn.Linear(in_features = self.in_node_dim, out_features = self.out_node_dim, device = device)\n",
    "\n",
    "        self.edge_film = FiLM(self.in_edge_dim, self.out_node_dim, device = device)\n",
    "\n",
    "    def forward(self, nodes : Tensor, edges : Tensor):\n",
    "        queries = self.lin_query(nodes) # batch_size x num_nodes x out_node_dim\n",
    "        keys = self.lin_key(nodes)      # batch_size x num_nodes x out_node_dim\n",
    "        values = self.lin_value(nodes)  # batch_size x num_nodes x out_node_dim\n",
    "\n",
    "        # Outer Product Attention -------\n",
    "        queries = queries.unsqueeze(2)                                       # batch_size x num_nodes x 1 x out_node_dim (each element in batch is a column tensor where each element is a row? vector)\n",
    "        keys = keys.unsqueeze(1)                                             # batch_size x 1 x num_nodes x out_node_dims (each element in batch is a row tensor where each element is a row? vector)\n",
    "        attention = queries * keys / math.sqrt(self.out_node_dim)  # batch_size x num_nodes x num_nodes x out_node_dim (each element in batch is a 2d tensor where each element is a row? vector)\n",
    "\n",
    "        # Incorporate edge features into attention\n",
    "        edge_attention = self.edge_film(edges, attention) # batch_size x num_nodes x num_nodes x out_node_dim (each element in batch is a 2d tensor, where each element in 2d tensor is a row? vector)\n",
    "\n",
    "        # Normalize attention; Digress uses softmax to normalize but the original outer product attention paper uses tanh\n",
    "        normalized_attention = torch.softmax(input = edge_attention, dim = 3) # element wise softmax, so every element vector is normalized\n",
    "\n",
    "        # Weight node representations\n",
    "        values = values.unsqueeze(1)                               # batch_size x 1 x num_nodes x out_node_dims\n",
    "        weighted_values = normalized_attention.permute(0, 1, 3, 2) @ values  # batch_size x num_nodes x out_node_dim x out_node_dim\n",
    "\n",
    "        return weighted_values, edge_attention\n",
    "\n",
    "# Feature-Wise Linear Modulation Layer\n",
    "class FiLM(nn.Module):\n",
    "    def __init__(self, a_feature_dim : int, b_feature_dim : int, device : torch.device):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lin1 = nn.Linear(in_features = a_feature_dim, out_features = b_feature_dim, device = device)\n",
    "        self.lin2 = nn.Linear(in_features = a_feature_dim, out_features = b_feature_dim, device = device)\n",
    "    \n",
    "    def forward(self, a : Tensor, b : Tensor):\n",
    "        mul = self.lin1(a)\n",
    "        add = self.lin2(a)\n",
    "\n",
    "        # For vanilla FiLM you are only supposed to do mul * b + add\n",
    "        # I assume digress put the additional '+ b' as a skip connection\n",
    "        return mul * b + add + b\n",
    "\n",
    "# Principal Neighbourhood Aggregation Layer\n",
    "class PNA(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.mean_aggr = MeanAggregation()\n",
    "        # self.max_aggr = MaxAggregation()\n",
    "        # self.min_aggr = MinAggregation()\n",
    "        # self.std_aggr = StdAggregation()\n",
    "    \n",
    "    def forward(self, input : Tensor, dim):\n",
    "        # find the biggest element in dim\n",
    "        mean = torch.mean(input, dim)\n",
    "        max = torch.amax(input, dim)\n",
    "        min = torch.amin(input, dim)\n",
    "        stdev = torch.std(input, dim) # Standard Deviation\n",
    "\n",
    "        return torch.cat(tensors = (mean, max, min, stdev), dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os.path\n",
    "os.chdir('SketchGraphs/')\n",
    "import sketchgraphs.data as datalib\n",
    "os.chdir('../')\n",
    "from dataset import SketchDataset\n",
    "\n",
    "model = TransformerDecoder(NODE_FEATURE_DIMENSION, EDGE_FEATURE_DIMENSION, 512, 4, 4, 'cuda')\n",
    "latents = torch.randn(size = (2,512), device = 'cuda')\n",
    "pred_nodes, pred_edges = model(latents)\n",
    "dataset = SketchDataset(root=\"data/\")\n",
    "s1 = dataset.preds_to_sketch(pred_nodes[0].detach().cpu(), pred_edges[0].detach().cpu())\n",
    "s2 = dataset.preds_to_sketch(pred_nodes[1].detach().cpu(), pred_edges[1].detach().cpu())\n",
    "datalib.render_sketch(s1)\n",
    "datalib.render_sketch(s2)\n",
    "# model = GVAE(device)\n",
    "# dataset = SketchDataset(root=\"data/\")\n",
    "# dataset.len()\n",
    "\n",
    "# nodes, edges, node_params_mask = dataset[0]\n",
    "\n",
    "\n",
    "# s2 = dataset.preds_to_sketch(nodes, edges)\n",
    "\n",
    "# datalib.render_sketch(s2)\n",
    "\n",
    "# print(s2)\n",
    "\n",
    "# # batch_size = 64\n",
    "# # dataloader = DataLoader(dataset = dataset, batch_size = batch_size, shuffle = True)\n",
    "# # learning_rate = 1e-2\n",
    "# # epochs = 4\n",
    "# # optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n",
    "# # model.train()\n",
    "# # for batch, (nodes, edges, node_params_mask) in enumerate(dataloader):\n",
    "# #     optimizer.zero_grad()\n",
    "    \n",
    "# #     target_nodes = nodes.to(device)\n",
    "# #     target_edges = edges.to(device)\n",
    "# #     node_params_mask = node_params_mask.to(device)\n",
    "\n",
    "# #     pred_nodes, pred_edges, means, logvars = model(target_nodes, target_edges)\n",
    "\n",
    "# #     loss = reconstruction_loss(pred_nodes, pred_edges, target_nodes, target_edges, node_params_mask) + kl_loss(means, logvars)\n",
    "\n",
    "# #     loss.backward()\n",
    "# #     optimizer.step()\n",
    "\n",
    "# #     print(f\"loss: {loss.item()}\")\n",
    "\n",
    "\n",
    "# model.eval()\n",
    "# nodes, edges = model.sample_graph()\n",
    "# sketch = SketchDataset.preds_to_sketch(nodes.cpu(), edges.cpu())\n",
    "# print(sketch)\n",
    "# datalib.render_sketch(sketch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = torch.randint(0, 10, size = (2, 3, 4))\n",
    "K = torch.randint(0, 10, size = (2, 3, 4))\n",
    "V = torch.randint(0, 10, size = (2, 3, 4))\n",
    "\n",
    "temp = Q.unsqueeze(0) * K\n",
    "x = 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
