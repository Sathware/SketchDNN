{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "# from torch.profiler import profile, record_function, ProfilerActivity\n",
    "import torch\n",
    "from model import GVAE\n",
    "from loss import reconstruction_loss, kl_loss\n",
    "from dataset import SketchDataset\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "os.chdir('SketchGraphs/')\n",
    "import sketchgraphs.data as datalib\n",
    "os.chdir('../')\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiGPUTrainer:\n",
    "    def __init__(\n",
    "            self,\n",
    "            model: torch.nn.Module,\n",
    "            train_set: Subset,\n",
    "            validate_set: Subset,\n",
    "            optimizer: torch.optim.Optimizer,\n",
    "            scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
    "            gpu_id: int,\n",
    "            num_epochs: int,\n",
    "            experiment_string: str,\n",
    "            batch_size: int\n",
    "            ):\n",
    "        model.device = gpu_id\n",
    "        self.model = model.to(gpu_id)\n",
    "        self.train_loader = DataLoader(dataset = train_set, \n",
    "                                       batch_size = batch_size, \n",
    "                                       shuffle = True\n",
    "                                      )\n",
    "        self.validate_loader = DataLoader(dataset = validate_set, \n",
    "                                          batch_size = batch_size, \n",
    "                                          shuffle = True\n",
    "                                         )\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.gpu_id = gpu_id\n",
    "        self.writer = SummaryWriter(f'runs/{experiment_string}')\n",
    "        self.num_epochs = num_epochs\n",
    "\n",
    "        self.global_step = 0\n",
    "        self.curr_epoch = 0\n",
    "        self.min_validation_loss = float('inf')\n",
    "\n",
    "    \n",
    "    def train_batch(self, nodes : torch.Tensor, edges : torch.Tensor, node_params_mask : torch.Tensor) -> float:\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        nodes = nodes.to(self.gpu_id)\n",
    "        edges = edges.to(self.gpu_id)\n",
    "        node_params_mask = node_params_mask.to(self.gpu_id)\n",
    "\n",
    "        pred_nodes, pred_edges, means, logvars = self.model(nodes, edges)\n",
    "\n",
    "        assert pred_nodes.isfinite().all(), \"Model output for nodes has non finite values\"\n",
    "        assert pred_edges.isfinite().all(), \"Model output for edges has non finite values\"\n",
    "        assert means.isfinite().all(),      \"Model output for means has non finite values\"\n",
    "        assert logvars.isfinite().all(),    \"Model output for logvars has non finite values\"\n",
    "\n",
    "        loss = reconstruction_loss(pred_nodes, pred_edges, nodes, edges, node_params_mask)\n",
    "        # loss += 0.1*kl_loss(means, logvars)\n",
    "\n",
    "        assert loss.isfinite().all(), \"Loss is non finite value\"\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "    \n",
    "    def train_epoch(self):\n",
    "        pbar = tqdm(self.train_loader)\n",
    "        for nodes, edges, node_params_mask in pbar:\n",
    "            iter_loss = self.train_batch(nodes, edges, node_params_mask)\n",
    "\n",
    "            self.global_step += 1\n",
    "\n",
    "            if (self.global_step % 10 == 9):\n",
    "                if self.gpu_id == 0: self.writer.add_scalar(\"Training Loss\", iter_loss, self.global_step)\n",
    "                self.scheduler.step(iter_loss)\n",
    "            \n",
    "            pbar.set_description(f\"Training Epoch {self.curr_epoch} Iter Loss: {iter_loss}  \")\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def validate(self):\n",
    "        pbar = tqdm(self.validate_loader)\n",
    "        total_loss = 0\n",
    "        for nodes, edges, node_params_mask in pbar:\n",
    "            nodes = nodes.to(self.gpu_id)\n",
    "            edges = edges.to(self.gpu_id)\n",
    "            node_params_mask = node_params_mask.to(self.gpu_id)\n",
    "\n",
    "            pred_nodes, pred_edges, means, logvars = self.model(nodes, edges)\n",
    "\n",
    "            loss = reconstruction_loss(pred_nodes, pred_edges, nodes, edges, node_params_mask)\n",
    "            # loss += 0.1*kl_loss(means, logvars)\n",
    "\n",
    "            total_loss += loss\n",
    "\n",
    "            assert loss.isfinite().all(), \"Loss is non finite value\"\n",
    "\n",
    "            pbar.set_description(f\"Validating Epoch {self.curr_epoch}  \")\n",
    "        \n",
    "        avg_loss = total_loss / len(pbar)\n",
    "        if avg_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = avg_loss\n",
    "            if self.gpu_id == 0: \n",
    "                self.writer.add_scalar(\"Validation Loss\", avg_loss, self.curr_epoch)\n",
    "                self.save_checkpoint()\n",
    "        \n",
    "        if self.gpu_id == 0:\n",
    "            fig, axes = plt.subplots(nrows = 4, ncols = 2, figsize=(8, 16))\n",
    "            fig.suptitle(f\"Target (left) vs Preds (right) for epoch {self.curr_epoch}\")\n",
    "            for i in range(4):\n",
    "                target_sketch = SketchDataset.preds_to_sketch(nodes[i].cpu(), edges[i].cpu())\n",
    "                pred_sketch = SketchDataset.preds_to_sketch(pred_nodes[i].cpu(), pred_edges[i].cpu())\n",
    "                \n",
    "                datalib.render_sketch(target_sketch, axes[i, 0])\n",
    "                datalib.render_sketch(pred_sketch, axes[i, 1])\n",
    "            \n",
    "            self.writer.add_figure(f\"Epoch result visualization\", fig, self.curr_epoch)\n",
    "            plt.close()\n",
    "                \n",
    "    def train(self):\n",
    "        self.global_step = 0\n",
    "        self.curr_epoch = 0\n",
    "\n",
    "        while (self.curr_epoch < self.num_epochs):\n",
    "            self.model.train()\n",
    "            self.train_epoch()\n",
    "            self.model.eval()\n",
    "            self.validate()\n",
    "            self.curr_epoch += 1\n",
    "    \n",
    "    def save_checkpoint(self):\n",
    "        checkpoint = self.model.state_dict()\n",
    "        torch.save(checkpoint, \"best_model_checkpoint.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = SketchDataset(root=\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Graphs in total:  3981513\n",
      "Number of Graphs for training:  3583362\n",
      "Number of Graphs for validation:  131390\n",
      "Number of Graphs for testing:  266761\n"
     ]
    }
   ],
   "source": [
    "train_set, validate_set, test_set = random_split(dataset = dataset, lengths = [0.9, 0.033, 0.067])\n",
    "\n",
    "print(\"Number of Graphs in total: \", len(dataset))\n",
    "print(\"Number of Graphs for training: \", len(train_set))\n",
    "print(\"Number of Graphs for validation: \", len(validate_set))\n",
    "print(\"Number of Graphs for testing: \", len(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 768\n",
    "learning_rate = 1e-10\n",
    "num_epochs = 50\n",
    "experiment_string = \"gvae_experiment_ddp_2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GVAE(device)\n",
    "# if os.path.exists(f\"best_model_checkpoint.pth\"):\n",
    "   #  model.load_state_dict(torch.load(f\"best_model_checkpoint.pth\"))\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = MultiGPUTrainer(\n",
    "    model = model,\n",
    "    train_set = train_set,\n",
    "    validate_set = validate_set,\n",
    "    optimizer = optimizer,\n",
    "    scheduler = scheduler,\n",
    "    gpu_id = 0,\n",
    "    num_epochs = num_epochs,\n",
    "    experiment_string = experiment_string,\n",
    "    batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on_multiple_gpus(rank: int, world_size: int):\n",
    "    MultiGPUTrainer.ddp_setup(rank, world_size)\n",
    "\n",
    "    model = GVAE(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = \"min\", patience = 5)\n",
    "\n",
    "    trainer = MultiGPUTrainer(\n",
    "        model = model,\n",
    "        train_set = train_set,\n",
    "        validate_set = validate_set,\n",
    "        optimizer = optimizer,\n",
    "        scheduler = scheduler,\n",
    "        gpu_id = rank,\n",
    "        num_epochs = num_epochs,\n",
    "        experiment_string = experiment_string,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = 32\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    destroy_process_group()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Graphs in total:  3981513\n",
      "Number of Graphs for training:  3583362\n",
      "Number of Graphs for validation:  131390\n",
      "Number of Graphs for testing:  266761\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.multiprocessing as mp\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from torch.utils.data import DataLoader, Subset, random_split, TensorDataset\n",
    "from dataset import SketchDataset\n",
    "from model import GVAE\n",
    "from loss import reconstruction_loss, kl_loss\n",
    "from distributed_trainer import MultiGPUTrainer, train_on_multiple_gpus\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "dataset = SketchDataset(root=\"data/\")\n",
    "dataset = TensorDataset(dataset.nodes, dataset.edges)\n",
    "train_set, validate_set, test_set = random_split(dataset = dataset, lengths = [0.9, 0.033, 0.067])\n",
    "\n",
    "print(\"Number of Graphs in total: \", len(dataset))\n",
    "print(\"Number of Graphs for training: \", len(train_set))\n",
    "print(\"Number of Graphs for validation: \", len(validate_set))\n",
    "print(\"Number of Graphs for testing: \", len(test_set))\n",
    "\n",
    "batch_size = 480\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 25\n",
    "experiment_string = \"gvae_ddp_embedsize_1024_num_head_8_num_tflayers_4_no_batchlayernorm_no_mseloss_no_edgeloss\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/934 [00:00<?, ?it/s][W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "[W reducer.cpp:1300] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())\n",
      "Training Epoch 0 Iter Loss: 2.0518107414245605  : 100%|██████████| 934/934 [14:07<00:00,  1.10it/s]]\n",
      "Training Epoch 0 Iter Loss: 2.059163808822632  : 100%|██████████| 934/934 [14:07<00:00,  1.10it/s]]\n",
      "\n",
      "Training Epoch 0 Iter Loss: 2.0024807453155518  : 100%|██████████| 934/934 [14:07<00:00,  1.10it/s]\n",
      "Training Epoch 0 Iter Loss: 2.0492944717407227  : 100%|█████████▉| 933/934 [14:07<00:00,  1.10it/s]\n",
      "Training Epoch 0 Iter Loss: 2.0014264583587646  : 100%|██████████| 934/934 [14:07<00:00,  1.10it/s]\n",
      "Training Epoch 0 Iter Loss: 2.0492944717407227  : 100%|██████████| 934/934 [14:07<00:00,  1.10it/s]\n",
      "Training Epoch 0 Iter Loss: 2.0202505588531494  : 100%|██████████| 934/934 [14:07<00:00,  1.10it/s]\n",
      "Validating Epoch 0  : 100%|██████████| 35/35 [00:12<00:00,  2.85it/s]\n",
      "Validating Epoch 0  : 100%|██████████| 35/35 [00:12<00:00,  2.84it/s]\n",
      "Validating Epoch 0  : 100%|██████████| 35/35 [00:12<00:00,  2.78it/s]\n",
      "Validating Epoch 0  : 100%|██████████| 35/35 [00:12<00:00,  2.78it/s]\n",
      "Validating Epoch 0  : 100%|██████████| 35/35 [00:12<00:00,  2.79it/s]\n",
      "Validating Epoch 0  : 100%|██████████| 35/35 [00:12<00:00,  2.77it/s]\n",
      "Validating Epoch 0  : 100%|██████████| 35/35 [00:12<00:00,  2.72it/s]\n",
      "Validating Epoch 0  : 100%|██████████| 35/35 [00:12<00:00,  2.71it/s]\n",
      "Training Epoch 1 Iter Loss: 1.8890092372894287  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]s]\n",
      "Training Epoch 1 Iter Loss: 1.911556601524353  : 100%|█████████▉| 933/934 [14:02<00:00,  1.12it/s] \n",
      "Training Epoch 1 Iter Loss: 1.896315574645996  : 100%|██████████| 934/934 [13:55<00:00,  1.12it/s]]\n",
      "Training Epoch 1 Iter Loss: 1.82477605342865  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]s]\n",
      "Training Epoch 1 Iter Loss: 1.911556601524353  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 1 Iter Loss: 1.8524270057678223  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 1 Iter Loss: 1.8253436088562012  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 1 Iter Loss: 1.9017921686172485  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Validating Epoch 1  : 100%|██████████| 35/35 [00:12<00:00,  2.86it/s]\n",
      "Validating Epoch 1  : 100%|██████████| 35/35 [00:12<00:00,  2.84it/s]\n",
      "Validating Epoch 1  : 100%|██████████| 35/35 [00:12<00:00,  2.83it/s]\n",
      "Validating Epoch 1  : 100%|██████████| 35/35 [00:12<00:00,  2.83it/s]\n",
      "Validating Epoch 1  : 100%|██████████| 35/35 [00:12<00:00,  2.82it/s]\n",
      "Validating Epoch 1  : 100%|██████████| 35/35 [00:12<00:00,  2.77it/s]\n",
      "Validating Epoch 1  : 100%|██████████| 35/35 [00:12<00:00,  2.73it/s]\n",
      "Validating Epoch 1  : 100%|██████████| 35/35 [00:12<00:00,  2.69it/s]\n",
      "Training Epoch 2 Iter Loss: 1.8080337047576904  : 100%|██████████| 934/934 [13:55<00:00,  1.12it/s]\n",
      "Training Epoch 2 Iter Loss: 1.7822996377944946  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s]\n",
      "Training Epoch 2 Iter Loss: 1.7191232442855835  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s]\n",
      "Training Epoch 2 Iter Loss: 1.7995246648788452  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s]\n",
      "Training Epoch 2 Iter Loss: 1.8273259401321411  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s]\n",
      "Training Epoch 2 Iter Loss: 1.8159546852111816  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s]\n",
      "Training Epoch 2 Iter Loss: 1.7188061475753784  : 100%|██████████| 934/934 [13:58<00:00,  1.11it/s]\n",
      "Training Epoch 2 Iter Loss: 1.752946138381958  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s]\n",
      "Validating Epoch 2  : 100%|██████████| 35/35 [00:12<00:00,  2.88it/s]\n",
      "Validating Epoch 2  : 100%|██████████| 35/35 [00:12<00:00,  2.82it/s]\n",
      "Validating Epoch 2  : 100%|██████████| 35/35 [00:12<00:00,  2.82it/s]\n",
      "Validating Epoch 2  : 100%|██████████| 35/35 [00:12<00:00,  2.81it/s]\n",
      "Validating Epoch 2  : 100%|██████████| 35/35 [00:12<00:00,  2.80it/s]\n",
      "Validating Epoch 2  : 100%|██████████| 35/35 [00:12<00:00,  2.80it/s]\n",
      "Validating Epoch 2  : 100%|██████████| 35/35 [00:12<00:00,  2.78it/s]\n",
      "Validating Epoch 2  : 100%|██████████| 35/35 [00:12<00:00,  2.78it/s]\n",
      "Training Epoch 3 Iter Loss: 1.662300705909729  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s] \n",
      "Training Epoch 3 Iter Loss: 1.6607139110565186  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s]\n",
      "Training Epoch 3 Iter Loss: 1.7823021411895752  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s]\n",
      "Training Epoch 3 Iter Loss: 1.7707891464233398  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Training Epoch 3 Iter Loss: 1.6990485191345215  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s]\n",
      "Training Epoch 3 Iter Loss: 1.760702133178711  : 100%|██████████| 934/934 [13:54<00:00,  1.12it/s]\n",
      "Training Epoch 3 Iter Loss: 1.752342700958252  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s] \n",
      "Training Epoch 3 Iter Loss: 1.731688141822815  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s]\n",
      "Validating Epoch 3  : 100%|██████████| 35/35 [00:12<00:00,  2.79it/s]\n",
      "Validating Epoch 3  : 100%|██████████| 35/35 [00:12<00:00,  2.73it/s]\n",
      "Validating Epoch 3  : 100%|██████████| 35/35 [00:12<00:00,  2.74it/s]\n",
      "Validating Epoch 3  : 100%|██████████| 35/35 [00:12<00:00,  2.72it/s]\n",
      "Validating Epoch 3  : 100%|██████████| 35/35 [00:12<00:00,  2.70it/s]\n",
      "Validating Epoch 3  : 100%|██████████| 35/35 [00:12<00:00,  2.70it/s]\n",
      "Validating Epoch 3  : 100%|██████████| 35/35 [00:12<00:00,  2.69it/s]\n",
      "Validating Epoch 3  : 100%|██████████| 35/35 [00:13<00:00,  2.69it/s]\n",
      "Training Epoch 4 Iter Loss: 1.7579416036605835  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]]\n",
      "Training Epoch 4 Iter Loss: 1.6304069757461548  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 4 Iter Loss: 1.6707565784454346  : 100%|██████████| 934/934 [14:01<00:00,  1.28it/s]\n",
      "Training Epoch 4 Iter Loss: 1.7353712320327759  : 100%|██████████| 934/934 [13:56<00:00,  1.12it/s]\n",
      "Training Epoch 4 Iter Loss: 1.6707565784454346  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 4 Iter Loss: 1.7044248580932617  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 4 Iter Loss: 1.6334736347198486  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 4 Iter Loss: 1.7471225261688232  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Validating Epoch 4  : 100%|██████████| 35/35 [00:12<00:00,  2.83it/s]\n",
      "Validating Epoch 4  : 100%|██████████| 35/35 [00:12<00:00,  2.81it/s]\n",
      "Validating Epoch 4  : 100%|██████████| 35/35 [00:12<00:00,  2.77it/s]\n",
      "Validating Epoch 4  : 100%|██████████| 35/35 [00:12<00:00,  2.70it/s]\n",
      "Validating Epoch 4  : 100%|██████████| 35/35 [00:12<00:00,  2.70it/s]\n",
      "Validating Epoch 4  : 100%|██████████| 35/35 [00:13<00:00,  2.68it/s]\n",
      "Validating Epoch 4  : 100%|██████████| 35/35 [00:13<00:00,  2.65it/s]\n",
      "Validating Epoch 4  : 100%|██████████| 35/35 [00:13<00:00,  2.62it/s]\n",
      "Training Epoch 5 Iter Loss: 1.7436586618423462  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]]\n",
      "Training Epoch 5 Iter Loss: 1.6187553405761719  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 5 Iter Loss: 1.7140235900878906  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 5 Iter Loss: 1.6142534017562866  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 5 Iter Loss: 1.655360221862793  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s] \n",
      "Training Epoch 5 Iter Loss: 1.7338848114013672  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 5 Iter Loss: 1.7208731174468994  : 100%|██████████| 934/934 [13:56<00:00,  1.12it/s]\n",
      "Training Epoch 5 Iter Loss: 1.6888227462768555  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Validating Epoch 5  : 100%|██████████| 35/35 [00:12<00:00,  2.85it/s]\n",
      "Validating Epoch 5  : 100%|██████████| 35/35 [00:12<00:00,  2.85it/s]\n",
      "Validating Epoch 5  : 100%|██████████| 35/35 [00:12<00:00,  2.82it/s]\n",
      "Validating Epoch 5  : 100%|██████████| 35/35 [00:12<00:00,  2.80it/s]\n",
      "Validating Epoch 5  : 100%|██████████| 35/35 [00:12<00:00,  2.81it/s]\n",
      "Validating Epoch 5  : 100%|██████████| 35/35 [00:12<00:00,  2.80it/s]\n",
      "Validating Epoch 5  : 100%|██████████| 35/35 [00:12<00:00,  2.79it/s]\n",
      "Validating Epoch 5  : 100%|██████████| 35/35 [00:12<00:00,  2.80it/s]\n",
      "Training Epoch 6 Iter Loss: 1.7060095071792603  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]]]\n",
      "Training Epoch 6 Iter Loss: 1.7346751689910889  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 6 Iter Loss: 1.7259770631790161  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 6 Iter Loss: 1.7120083570480347  : 100%|██████████| 934/934 [13:56<00:00,  1.12it/s]\n",
      "Training Epoch 6 Iter Loss: 1.6110116243362427  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 6 Iter Loss: 1.6792908906936646  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "\n",
      "Training Epoch 6 Iter Loss: 1.6052526235580444  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Validating Epoch 6  : 100%|██████████| 35/35 [00:12<00:00,  2.81it/s]\n",
      "Validating Epoch 6  : 100%|██████████| 35/35 [00:12<00:00,  2.80it/s]\n",
      "Validating Epoch 6  : 100%|██████████| 35/35 [00:12<00:00,  2.78it/s]\n",
      "Validating Epoch 6  : 100%|██████████| 35/35 [00:12<00:00,  2.74it/s]\n",
      "Validating Epoch 6  : 100%|██████████| 35/35 [00:12<00:00,  2.71it/s]\n",
      "Validating Epoch 6  : 100%|██████████| 35/35 [00:12<00:00,  2.70it/s]\n",
      "Validating Epoch 6  : 100%|██████████| 35/35 [00:12<00:00,  2.70it/s]\n",
      "Validating Epoch 6  : 100%|██████████| 35/35 [00:13<00:00,  2.66it/s]\n",
      "Training Epoch 7 Iter Loss: 1.7210339307785034  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]s]\n",
      "Training Epoch 7 Iter Loss: 1.600009560585022  : 100%|██████████| 934/934 [14:01<00:00,  1.28it/s]\n",
      "Training Epoch 7 Iter Loss: 1.6412155628204346  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 7 Iter Loss: 1.600009560585022  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 7 Iter Loss: 1.6068111658096313  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 7 Iter Loss: 1.7010914087295532  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 7 Iter Loss: 1.673171877861023  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 7 Iter Loss: 1.7063307762145996  : 100%|██████████| 934/934 [13:56<00:00,  1.12it/s]\n",
      "Validating Epoch 7  : 100%|██████████| 35/35 [00:12<00:00,  2.81it/s]\n",
      "Validating Epoch 7  : 100%|██████████| 35/35 [00:12<00:00,  2.80it/s]\n",
      "Validating Epoch 7  : 100%|██████████| 35/35 [00:12<00:00,  2.80it/s]\n",
      "Validating Epoch 7  : 100%|██████████| 35/35 [00:12<00:00,  2.80it/s]\n",
      "Validating Epoch 7  : 100%|██████████| 35/35 [00:12<00:00,  2.80it/s]\n",
      "Validating Epoch 7  : 100%|██████████| 35/35 [00:12<00:00,  2.75it/s]\n",
      "Validating Epoch 7  : 100%|██████████| 35/35 [00:12<00:00,  2.75it/s]\n",
      "Validating Epoch 7  : 100%|██████████| 35/35 [00:12<00:00,  2.74it/s]\n",
      "Training Epoch 8 Iter Loss: 1.6979657411575317  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]\n",
      "Training Epoch 8 Iter Loss: 1.7178372144699097  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]\n",
      "Training Epoch 8 Iter Loss: 1.7247626781463623  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]\n",
      "Training Epoch 8 Iter Loss: 1.6378861665725708  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]\n",
      "Training Epoch 8 Iter Loss: 1.6044639348983765  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]\n",
      "Training Epoch 8 Iter Loss: 1.7025668621063232  : 100%|██████████| 934/934 [13:57<00:00,  1.11it/s]\n",
      "Training Epoch 8 Iter Loss: 1.5968194007873535  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]\n",
      "\n",
      "Validating Epoch 8  : 100%|██████████| 35/35 [00:11<00:00,  2.94it/s]\n",
      "Validating Epoch 8  : 100%|██████████| 35/35 [00:12<00:00,  2.88it/s]\n",
      "Validating Epoch 8  : 100%|██████████| 35/35 [00:12<00:00,  2.88it/s]\n",
      "Validating Epoch 8  : 100%|██████████| 35/35 [00:12<00:00,  2.88it/s]\n",
      "Validating Epoch 8  : 100%|██████████| 35/35 [00:12<00:00,  2.86it/s]\n",
      "Validating Epoch 8  : 100%|██████████| 35/35 [00:12<00:00,  2.86it/s]\n",
      "Validating Epoch 8  : 100%|██████████| 35/35 [00:12<00:00,  2.86it/s]\n",
      "Validating Epoch 8  : 100%|██████████| 35/35 [00:12<00:00,  2.86it/s]\n",
      "Training Epoch 9 Iter Loss: 1.7157100439071655  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]]\n",
      "Training Epoch 9 Iter Loss: 1.721975326538086  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]]\n",
      "Training Epoch 9 Iter Loss: 1.635719895362854  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]]\n",
      "Training Epoch 9 Iter Loss: 1.695919394493103  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]\n",
      "Training Epoch 9 Iter Loss: 1.6031172275543213  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]\n",
      "Training Epoch 9 Iter Loss: 1.6999980211257935  : 100%|██████████| 934/934 [13:57<00:00,  1.12it/s]\n",
      "Training Epoch 9 Iter Loss: 1.59479820728302  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]  \n",
      "Training Epoch 9 Iter Loss: 1.6662648916244507  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]\n",
      "Validating Epoch 9  : 100%|██████████| 35/35 [00:11<00:00,  2.94it/s]\n",
      "Validating Epoch 9  : 100%|██████████| 35/35 [00:12<00:00,  2.88it/s]\n",
      "Validating Epoch 9  : 100%|██████████| 35/35 [00:12<00:00,  2.85it/s]\n",
      "Validating Epoch 9  : 100%|██████████| 35/35 [00:12<00:00,  2.85it/s]\n",
      "Validating Epoch 9  : 100%|██████████| 35/35 [00:12<00:00,  2.81it/s]\n",
      "Validating Epoch 9  : 100%|██████████| 35/35 [00:12<00:00,  2.80it/s]\n",
      "Validating Epoch 9  : 100%|██████████| 35/35 [00:12<00:00,  2.78it/s]\n",
      "Validating Epoch 9  : 100%|██████████| 35/35 [00:12<00:00,  2.79it/s]\n",
      "Training Epoch 10 Iter Loss: 1.719986081123352  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]]\n",
      "\n",
      "Training Epoch 10 Iter Loss: 1.664259672164917  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]]\n",
      "Training Epoch 10 Iter Loss: 1.7142550945281982  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 10 Iter Loss: 1.593470573425293  : 100%|█████████▉| 933/934 [14:01<00:00,  1.12it/s] \n",
      "Training Epoch 10 Iter Loss: 1.6342607736587524  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 10 Iter Loss: 1.6023298501968384  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 10 Iter Loss: 1.593470573425293  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Validating Epoch 10  : 100%|██████████| 35/35 [00:12<00:00,  2.84it/s]\n",
      "Validating Epoch 10  : 100%|██████████| 35/35 [00:12<00:00,  2.79it/s]\n",
      "Validating Epoch 10  : 100%|██████████| 35/35 [00:12<00:00,  2.78it/s]\n",
      "Validating Epoch 10  : 100%|██████████| 35/35 [00:12<00:00,  2.77it/s]\n",
      "Validating Epoch 10  : 100%|██████████| 35/35 [00:12<00:00,  2.77it/s]\n",
      "Validating Epoch 10  : 100%|██████████| 35/35 [00:12<00:00,  2.76it/s]\n",
      "Validating Epoch 10  : 100%|██████████| 35/35 [00:12<00:00,  2.76it/s]\n",
      "Validating Epoch 10  : 100%|██████████| 35/35 [00:12<00:00,  2.70it/s]\n",
      "Training Epoch 11 Iter Loss: 1.7185194492340088  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Training Epoch 11 Iter Loss: 1.662795066833496  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]]\n",
      "\n",
      "Training Epoch 11 Iter Loss: 1.6332483291625977  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Training Epoch 11 Iter Loss: 1.6018704175949097  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 11 Iter Loss: 1.5925742387771606  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Training Epoch 11 Iter Loss: 1.6968839168548584  : 100%|██████████| 934/934 [13:56<00:00,  1.12it/s]\n",
      "Training Epoch 11 Iter Loss: 1.7132269144058228  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Validating Epoch 11  : 100%|██████████| 35/35 [00:12<00:00,  2.88it/s]\n",
      "Validating Epoch 11  : 100%|██████████| 35/35 [00:12<00:00,  2.86it/s]\n",
      "Validating Epoch 11  : 100%|██████████| 35/35 [00:12<00:00,  2.81it/s]\n",
      "Validating Epoch 11  : 100%|██████████| 35/35 [00:12<00:00,  2.81it/s]\n",
      "Validating Epoch 11  : 100%|██████████| 35/35 [00:12<00:00,  2.80it/s]\n",
      "Validating Epoch 11  : 100%|██████████| 35/35 [00:12<00:00,  2.77it/s]\n",
      "Validating Epoch 11  : 100%|██████████| 35/35 [00:12<00:00,  2.73it/s]\n",
      "Validating Epoch 11  : 100%|██████████| 35/35 [00:12<00:00,  2.71it/s]\n",
      "Training Epoch 12 Iter Loss: 1.6929060220718384  : 100%|██████████| 934/934 [14:04<00:00,  1.11it/s]s]\n",
      "Training Epoch 12 Iter Loss: 1.6959246397018433  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s]\n",
      "Training Epoch 12 Iter Loss: 1.712493658065796  : 100%|██████████| 934/934 [14:04<00:00,  1.11it/s] \n",
      "Training Epoch 12 Iter Loss: 1.5919452905654907  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]\n",
      "\n",
      "Training Epoch 12 Iter Loss: 1.7174323797225952  : 100%|██████████| 934/934 [14:04<00:00,  1.11it/s]\n",
      "Training Epoch 12 Iter Loss: 1.601594090461731  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]\n",
      "Training Epoch 12 Iter Loss: 1.6325269937515259  : 100%|██████████| 934/934 [14:04<00:00,  1.11it/s]\n",
      "Validating Epoch 12  : 100%|██████████| 35/35 [00:11<00:00,  2.93it/s]\n",
      "Validating Epoch 12  : 100%|██████████| 35/35 [00:11<00:00,  2.93it/s]\n",
      "Validating Epoch 12  : 100%|██████████| 35/35 [00:12<00:00,  2.92it/s]\n",
      "Validating Epoch 12  : 100%|██████████| 35/35 [00:12<00:00,  2.91it/s]\n",
      "Validating Epoch 12  : 100%|██████████| 35/35 [00:12<00:00,  2.89it/s]\n",
      "Validating Epoch 12  : 100%|██████████| 35/35 [00:12<00:00,  2.90it/s]\n",
      "Validating Epoch 12  : 100%|██████████| 35/35 [00:12<00:00,  2.85it/s]\n",
      "Validating Epoch 12  : 100%|██████████| 35/35 [00:12<00:00,  2.84it/s]\n",
      "Training Epoch 13 Iter Loss: 1.5914901494979858  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]\n",
      "Training Epoch 13 Iter Loss: 1.7166132926940918  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]\n",
      "Training Epoch 13 Iter Loss: 1.6924184560775757  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]\n",
      "Training Epoch 13 Iter Loss: 1.6952087879180908  : 100%|██████████| 934/934 [13:57<00:00,  1.11it/s]\n",
      "Training Epoch 13 Iter Loss: 1.7119603157043457  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]\n",
      "Training Epoch 13 Iter Loss: 1.6319997310638428  : 100%|██████████| 934/934 [14:03<00:00,  1.27it/s]\n",
      "Training Epoch 13 Iter Loss: 1.6319997310638428  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]\n",
      "Training Epoch 13 Iter Loss: 1.6014251708984375  : 100%|██████████| 934/934 [14:03<00:00,  1.11it/s]\n",
      "Validating Epoch 13  : 100%|██████████| 35/35 [00:11<00:00,  2.93it/s]\n",
      "Validating Epoch 13  : 100%|██████████| 35/35 [00:12<00:00,  2.84it/s]\n",
      "Validating Epoch 13  : 100%|██████████| 35/35 [00:12<00:00,  2.85it/s]\n",
      "Validating Epoch 13  : 100%|██████████| 35/35 [00:12<00:00,  2.84it/s]\n",
      "Validating Epoch 13  : 100%|██████████| 35/35 [00:12<00:00,  2.84it/s]\n",
      "Validating Epoch 13  : 100%|██████████| 35/35 [00:12<00:00,  2.83it/s]\n",
      "Validating Epoch 13  : 100%|██████████| 35/35 [00:12<00:00,  2.83it/s]\n",
      "Validating Epoch 13  : 100%|██████████| 35/35 [00:12<00:00,  2.76it/s]\n",
      "Training Epoch 14 Iter Loss: 1.7115639448165894  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]s]\n",
      "Training Epoch 14 Iter Loss: 1.6602245569229126  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 14 Iter Loss: 1.5911509990692139  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 14 Iter Loss: 1.6316052675247192  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 14 Iter Loss: 1.6013212203979492  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 14 Iter Loss: 1.6920583248138428  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 14 Iter Loss: 1.7159850597381592  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 14 Iter Loss: 1.6946643590927124  : 100%|██████████| 934/934 [13:57<00:00,  1.11it/s]\n",
      "Validating Epoch 14  : 100%|██████████| 35/35 [00:12<00:00,  2.81it/s]\n",
      "Validating Epoch 14  : 100%|██████████| 35/35 [00:12<00:00,  2.77it/s]\n",
      "Validating Epoch 14  : 100%|██████████| 35/35 [00:12<00:00,  2.72it/s]\n",
      "Validating Epoch 14  : 100%|██████████| 35/35 [00:12<00:00,  2.72it/s]\n",
      "Validating Epoch 14  : 100%|██████████| 35/35 [00:12<00:00,  2.70it/s]\n",
      "Validating Epoch 14  : 100%|██████████| 35/35 [00:13<00:00,  2.66it/s]\n",
      "Validating Epoch 14  : 100%|██████████| 35/35 [00:13<00:00,  2.63it/s]\n",
      "Validating Epoch 14  : 100%|██████████| 35/35 [00:13<00:00,  2.63it/s]\n",
      "Training Epoch 15 Iter Loss: 1.601257562637329  : 100%|██████████| 934/934 [14:01<00:00,  1.30it/s]]\n",
      "Training Epoch 15 Iter Loss: 1.601257562637329  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]]\n",
      "Training Epoch 15 Iter Loss: 1.5908927917480469  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 15 Iter Loss: 1.6313036680221558  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 15 Iter Loss: 1.6917868852615356  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 15 Iter Loss: 1.6942429542541504  : 100%|██████████| 934/934 [13:57<00:00,  1.12it/s]\n",
      "Training Epoch 15 Iter Loss: 1.7112642526626587  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 15 Iter Loss: 1.6597167253494263  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Validating Epoch 15  : 100%|██████████| 35/35 [00:11<00:00,  2.94it/s]\n",
      "Validating Epoch 15  : 100%|██████████| 35/35 [00:12<00:00,  2.89it/s]\n",
      "Validating Epoch 15  : 100%|██████████| 35/35 [00:12<00:00,  2.86it/s]\n",
      "Validating Epoch 15  : 100%|██████████| 35/35 [00:12<00:00,  2.86it/s]\n",
      "Validating Epoch 15  : 100%|██████████| 35/35 [00:12<00:00,  2.88it/s]\n",
      "Validating Epoch 15  : 100%|██████████| 35/35 [00:12<00:00,  2.83it/s]\n",
      "Validating Epoch 15  : 100%|██████████| 35/35 [00:12<00:00,  2.82it/s]\n",
      "Validating Epoch 15  : 100%|██████████| 35/35 [00:12<00:00,  2.82it/s]\n",
      "Training Epoch 16 Iter Loss: 1.7151079177856445  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]s]\n",
      "Training Epoch 16 Iter Loss: 1.6915781497955322  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "\n",
      "Training Epoch 16 Iter Loss: 1.7110340595245361  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Training Epoch 16 Iter Loss: 1.6939115524291992  : 100%|██████████| 934/934 [13:54<00:00,  1.12it/s]\n",
      "Training Epoch 16 Iter Loss: 1.6310696601867676  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Training Epoch 16 Iter Loss: 1.601219892501831  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 16 Iter Loss: 1.659309983253479  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Validating Epoch 16  : 100%|██████████| 35/35 [00:12<00:00,  2.87it/s]\n",
      "Validating Epoch 16  : 100%|██████████| 35/35 [00:12<00:00,  2.87it/s]\n",
      "Validating Epoch 16  : 100%|██████████| 35/35 [00:12<00:00,  2.86it/s]\n",
      "Validating Epoch 16  : 100%|██████████| 35/35 [00:12<00:00,  2.84it/s]\n",
      "Validating Epoch 16  : 100%|██████████| 35/35 [00:12<00:00,  2.83it/s]\n",
      "Validating Epoch 16  : 100%|██████████| 35/35 [00:12<00:00,  2.82it/s]\n",
      "Validating Epoch 16  : 100%|██████████| 35/35 [00:12<00:00,  2.83it/s]\n",
      "Validating Epoch 16  : 100%|██████████| 35/35 [00:12<00:00,  2.74it/s]\n",
      "Training Epoch 17 Iter Loss: 1.691415548324585  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s]]\n",
      "Training Epoch 17 Iter Loss: 1.7108529806137085  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s]\n",
      "Training Epoch 17 Iter Loss: 1.6011970043182373  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s]\n",
      "Training Epoch 17 Iter Loss: 1.6308846473693848  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s]\n",
      "Training Epoch 17 Iter Loss: 1.6589809656143188  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s]\n",
      "Training Epoch 17 Iter Loss: 1.590535044670105  : 100%|██████████| 934/934 [13:59<00:00,  1.11it/s]]\n",
      "Training Epoch 17 Iter Loss: 1.693648099899292  : 100%|██████████| 934/934 [13:55<00:00,  1.12it/s]\n",
      "Training Epoch 17 Iter Loss: 1.7147969007492065  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Validating Epoch 17  : 100%|██████████| 35/35 [00:12<00:00,  2.84it/s]\n",
      "Validating Epoch 17  : 100%|██████████| 35/35 [00:12<00:00,  2.82it/s]\n",
      "Validating Epoch 17  : 100%|██████████| 35/35 [00:12<00:00,  2.82it/s]\n",
      "Validating Epoch 17  : 100%|██████████| 35/35 [00:12<00:00,  2.81it/s]\n",
      "Validating Epoch 17  : 100%|██████████| 35/35 [00:12<00:00,  2.77it/s]\n",
      "Validating Epoch 17  : 100%|██████████| 35/35 [00:12<00:00,  2.76it/s]\n",
      "Validating Epoch 17  : 100%|██████████| 35/35 [00:12<00:00,  2.73it/s]\n",
      "Validating Epoch 17  : 100%|██████████| 35/35 [00:12<00:00,  2.73it/s]\n",
      "Training Epoch 18 Iter Loss: 1.5904085636138916  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Training Epoch 18 Iter Loss: 1.630735993385315  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]]\n",
      "Training Epoch 18 Iter Loss: 1.7145445346832275  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Training Epoch 18 Iter Loss: 1.7107104063034058  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Training Epoch 18 Iter Loss: 1.693434238433838  : 100%|██████████| 934/934 [13:55<00:00,  1.12it/s]]\n",
      "Training Epoch 18 Iter Loss: 1.6587110757827759  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Training Epoch 18 Iter Loss: 1.6011849641799927  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Training Epoch 18 Iter Loss: 1.6912857294082642  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Validating Epoch 18  : 100%|██████████| 35/35 [00:12<00:00,  2.81it/s]\n",
      "Validating Epoch 18  : 100%|██████████| 35/35 [00:12<00:00,  2.80it/s]\n",
      "Validating Epoch 18  : 100%|██████████| 35/35 [00:12<00:00,  2.75it/s]\n",
      "Validating Epoch 18  : 100%|██████████| 35/35 [00:12<00:00,  2.74it/s]\n",
      "Validating Epoch 18  : 100%|██████████| 35/35 [00:12<00:00,  2.74it/s]\n",
      "Validating Epoch 18  : 100%|██████████| 35/35 [00:12<00:00,  2.74it/s]\n",
      "Validating Epoch 18  : 100%|██████████| 35/35 [00:12<00:00,  2.73it/s]\n",
      "Validating Epoch 18  : 100%|██████████| 35/35 [00:13<00:00,  2.66it/s]\n",
      "Training Epoch 19 Iter Loss: 1.7143374681472778  : 100%|██████████| 934/934 [13:58<00:00,  1.11it/s]s]\n",
      "Training Epoch 19 Iter Loss: 1.6911823749542236  : 100%|██████████| 934/934 [13:58<00:00,  1.11it/s]\n",
      "Training Epoch 19 Iter Loss: 1.6932601928710938  : 100%|██████████| 934/934 [13:54<00:00,  1.12it/s]\n",
      "Training Epoch 19 Iter Loss: 1.6306158304214478  : 100%|██████████| 934/934 [13:58<00:00,  1.11it/s]\n",
      "Training Epoch 19 Iter Loss: 1.5903055667877197  : 100%|██████████| 934/934 [13:58<00:00,  1.11it/s]\n",
      "Training Epoch 19 Iter Loss: 1.7105956077575684  : 100%|██████████| 934/934 [13:58<00:00,  1.11it/s]\n",
      "Training Epoch 19 Iter Loss: 1.6011797189712524  : 100%|██████████| 934/934 [13:58<00:00,  1.11it/s]\n",
      "Training Epoch 19 Iter Loss: 1.6584876775741577  : 100%|██████████| 934/934 [13:58<00:00,  1.11it/s]\n",
      "Validating Epoch 19  : 100%|██████████| 35/35 [00:12<00:00,  2.87it/s]\n",
      "Validating Epoch 19  : 100%|██████████| 35/35 [00:12<00:00,  2.83it/s]\n",
      "Validating Epoch 19  : 100%|██████████| 35/35 [00:12<00:00,  2.76it/s]\n",
      "Validating Epoch 19  : 100%|██████████| 35/35 [00:12<00:00,  2.76it/s]\n",
      "Validating Epoch 19  : 100%|██████████| 35/35 [00:12<00:00,  2.76it/s]\n",
      "Validating Epoch 19  : 100%|██████████| 35/35 [00:13<00:00,  2.69it/s]\n",
      "Validating Epoch 19  : 100%|██████████| 35/35 [00:12<00:00,  2.70it/s]\n",
      "Validating Epoch 19  : 100%|██████████| 35/35 [00:12<00:00,  2.69it/s]\n",
      "Training Epoch 20 Iter Loss: 1.7141659259796143  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Training Epoch 20 Iter Loss: 1.59022057056427  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]s]\n",
      "Training Epoch 20 Iter Loss: 1.7105021476745605  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Training Epoch 20 Iter Loss: 1.6910977363586426  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 20 Iter Loss: 1.6011778116226196  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Training Epoch 20 Iter Loss: 1.6931158304214478  : 100%|██████████| 934/934 [13:56<00:00,  1.12it/s]\n",
      "Training Epoch 20 Iter Loss: 1.6305168867111206  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Training Epoch 20 Iter Loss: 1.658300757408142  : 100%|██████████| 934/934 [14:00<00:00,  1.11it/s]\n",
      "Validating Epoch 20  : 100%|██████████| 35/35 [00:12<00:00,  2.88it/s]\n",
      "Validating Epoch 20  : 100%|██████████| 35/35 [00:12<00:00,  2.86it/s]\n",
      "Validating Epoch 20  : 100%|██████████| 35/35 [00:12<00:00,  2.85it/s]\n",
      "Validating Epoch 20  : 100%|██████████| 35/35 [00:12<00:00,  2.85it/s]\n",
      "Validating Epoch 20  : 100%|██████████| 35/35 [00:12<00:00,  2.82it/s]\n",
      "Validating Epoch 20  : 100%|██████████| 35/35 [00:12<00:00,  2.80it/s]\n",
      "Validating Epoch 20  : 100%|██████████| 35/35 [00:12<00:00,  2.80it/s]\n",
      "Validating Epoch 20  : 100%|██████████| 35/35 [00:12<00:00,  2.79it/s]\n",
      "Training Epoch 21 Iter Loss: 1.6910284757614136  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]]\n",
      "Training Epoch 21 Iter Loss: 1.658143401145935  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]]\n",
      "\n",
      "Training Epoch 21 Iter Loss: 1.6304347515106201  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 21 Iter Loss: 1.7140220403671265  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 21 Iter Loss: 1.7104252576828003  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 21 Iter Loss: 1.6929960250854492  : 100%|██████████| 934/934 [13:57<00:00,  1.12it/s]\n",
      "Training Epoch 21 Iter Loss: 1.6011788845062256  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Validating Epoch 21  : 100%|██████████| 35/35 [00:12<00:00,  2.83it/s]\n",
      "Validating Epoch 21  : 100%|██████████| 35/35 [00:12<00:00,  2.77it/s]\n",
      "Validating Epoch 21  : 100%|██████████| 35/35 [00:12<00:00,  2.77it/s]\n",
      "Validating Epoch 21  : 100%|██████████| 35/35 [00:12<00:00,  2.76it/s]\n",
      "Validating Epoch 21  : 100%|██████████| 35/35 [00:12<00:00,  2.71it/s]\n",
      "Validating Epoch 21  : 100%|██████████| 35/35 [00:12<00:00,  2.71it/s]\n",
      "Validating Epoch 21  : 100%|██████████| 35/35 [00:13<00:00,  2.65it/s]\n",
      "Validating Epoch 21  : 100%|██████████| 35/35 [00:13<00:00,  2.66it/s]\n",
      "Training Epoch 22 Iter Loss: 1.6303658485412598  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 22 Iter Loss: 1.7103612422943115  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 22 Iter Loss: 1.6909712553024292  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 22 Iter Loss: 1.601181983947754  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s] \n",
      "Training Epoch 22 Iter Loss: 1.6928951740264893  : 100%|█████████▉| 933/934 [13:57<00:00,  1.12it/s]\n",
      "Training Epoch 22 Iter Loss: 1.6580098867416382  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 22 Iter Loss: 1.6928951740264893  : 100%|██████████| 934/934 [13:57<00:00,  1.12it/s]\n",
      "Training Epoch 22 Iter Loss: 1.590091347694397  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Validating Epoch 22  : 100%|██████████| 35/35 [00:12<00:00,  2.86it/s]\n",
      "Validating Epoch 22  : 100%|██████████| 35/35 [00:12<00:00,  2.84it/s]\n",
      "Validating Epoch 22  : 100%|██████████| 35/35 [00:12<00:00,  2.83it/s]\n",
      "Validating Epoch 22  : 100%|██████████| 35/35 [00:12<00:00,  2.79it/s]\n",
      "Validating Epoch 22  : 100%|██████████| 35/35 [00:12<00:00,  2.79it/s]\n",
      "Validating Epoch 22  : 100%|██████████| 35/35 [00:12<00:00,  2.78it/s]\n",
      "Validating Epoch 22  : 100%|██████████| 35/35 [00:12<00:00,  2.76it/s]\n",
      "Validating Epoch 22  : 100%|██████████| 35/35 [00:12<00:00,  2.76it/s]\n",
      "Training Epoch 23 Iter Loss: 1.7137984037399292  : 100%|█████████▉| 933/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 23 Iter Loss: 1.5900418758392334  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 23 Iter Loss: 1.7137984037399292  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 23 Iter Loss: 1.6928099393844604  : 100%|██████████| 934/934 [13:56<00:00,  1.12it/s]\n",
      "Training Epoch 23 Iter Loss: 1.6303077936172485  : 100%|██████████| 934/934 [14:01<00:00,  1.28it/s]\n",
      "Training Epoch 23 Iter Loss: 1.6578961610794067  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 23 Iter Loss: 1.6011852025985718  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Training Epoch 23 Iter Loss: 1.6303077936172485  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Validating Epoch 23  : 100%|██████████| 35/35 [00:13<00:00,  2.68it/s]\n",
      "Validating Epoch 23  : 100%|██████████| 35/35 [00:13<00:00,  2.67it/s]\n",
      "Validating Epoch 23  : 100%|██████████| 35/35 [00:13<00:00,  2.67it/s]\n",
      "Validating Epoch 23  : 100%|██████████| 35/35 [00:13<00:00,  2.68it/s]\n",
      "Validating Epoch 23  : 100%|██████████| 35/35 [00:13<00:00,  2.59it/s]\n",
      "Validating Epoch 23  : 100%|██████████| 35/35 [00:13<00:00,  2.59it/s]\n",
      "Validating Epoch 23  : 100%|██████████| 35/35 [00:13<00:00,  2.59it/s]\n",
      "Validating Epoch 23  : 100%|██████████| 35/35 [00:13<00:00,  2.54it/s]\n",
      "Training Epoch 24 Iter Loss: 1.690881371498108  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]]\n",
      "Training Epoch 24 Iter Loss: 1.7137112617492676  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 24 Iter Loss: 1.6302591562271118  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 24 Iter Loss: 1.5900001525878906  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 24 Iter Loss: 1.6011898517608643  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 24 Iter Loss: 1.7102630138397217  : 100%|██████████| 934/934 [14:02<00:00,  1.11it/s]\n",
      "Training Epoch 24 Iter Loss: 1.6927375793457031  : 100%|██████████| 934/934 [13:57<00:00,  1.11it/s]\n",
      "Training Epoch 24 Iter Loss: 1.6577990055084229  : 100%|██████████| 934/934 [14:01<00:00,  1.11it/s]\n",
      "Validating Epoch 24  : 100%|██████████| 35/35 [00:12<00:00,  2.72it/s]\n",
      "Validating Epoch 24  : 100%|██████████| 35/35 [00:12<00:00,  2.72it/s]\n",
      "Validating Epoch 24  : 100%|██████████| 35/35 [00:13<00:00,  2.66it/s]\n",
      "Validating Epoch 24  : 100%|██████████| 35/35 [00:13<00:00,  2.66it/s]\n",
      "Validating Epoch 24  : 100%|██████████| 35/35 [00:13<00:00,  2.66it/s]\n",
      "Validating Epoch 24  : 100%|██████████| 35/35 [00:13<00:00,  2.65it/s]\n",
      "Validating Epoch 24  : 100%|██████████| 35/35 [00:13<00:00,  2.61it/s]\n",
      "Validating Epoch 24  : 100%|██████████| 35/35 [00:13<00:00,  2.60it/s]\n"
     ]
    }
   ],
   "source": [
    "world_size = torch.cuda.device_count()\n",
    "mp.spawn(train_on_multiple_gpus, \n",
    "    args=(\n",
    "        world_size, \n",
    "        train_set, \n",
    "        validate_set, \n",
    "        learning_rate, \n",
    "        batch_size, \n",
    "        num_epochs, \n",
    "        experiment_string\n",
    "        ), \n",
    "    nprocs=world_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard.writer import SummaryWriter\n",
    "\n",
    "train_loader = DataLoader(dataset = train_set, batch_size = batch_size, shuffle = True)\n",
    "validate_loader = DataLoader(dataset = validate_set, batch_size = batch_size, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_set, batch_size = batch_size, shuffle = True)\n",
    "#torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "\n",
    "\n",
    "\n",
    "# validate_batches = [batch for batch in tqdm(DataLoader(dataset = validate_set, batch_size = batch_size, shuffle = True, persistent_workers = True, num_workers = 32))]\n",
    "# test_batches = [batch for batch in tqdm(DataLoader(dataset = test_set, batch_size = batch_size, shuffle = True, persistent_workers = True, num_workers = 32))]\n",
    "\n",
    "model = GVAE(device)\n",
    "# if os.path.exists(f\"best_model_checkpoint.pth\"):\n",
    "   #  model.load_state_dict(torch.load(f\"best_model_checkpoint.pth\"))\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 5)\n",
    "\n",
    "writer = SummaryWriter('runs/gvae_experiment_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "best_validation_loss = float('inf')# validate_model(model, validate_loader, writer)\n",
    "global_step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    # Train Model for one epoch\n",
    "    model.train()\n",
    "    total_train_loss = 0.0\n",
    "    num_train_batches = 0\n",
    "    pbar = tqdm(train_loader)\n",
    "    for target_nodes, target_edges in pbar:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        target_nodes = target_nodes.to(device)\n",
    "        target_edges = target_edges.to(device)\n",
    "\n",
    "        pred_nodes, pred_edges, means, logvars = model(target_nodes, target_edges)\n",
    "\n",
    "        if (not pred_nodes.isfinite().all()):\n",
    "            raise ValueError(\"pred nodes is not finite\")\n",
    "        if (not pred_edges.isfinite().all()):\n",
    "            raise ValueError(\"pred edges is not finite\")\n",
    "        if (not means.isfinite().all()):\n",
    "            raise ValueError(\"means is not finite\")\n",
    "        if (not logvars.isfinite().all()):\n",
    "            raise ValueError(\"logvars is not finite\")\n",
    "        \n",
    "        loss = reconstruction_loss(pred_nodes, pred_edges, target_nodes, target_edges) \n",
    "        loss += 0.1*kl_loss(means, logvars)\n",
    "        \n",
    "        if (not loss.isfinite().all()):\n",
    "            raise ValueError(\"Loss is not finite\")\n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        pbar.set_description(f\"Epoch {epoch} --Training-- Iter loss: {loss.item()} -\")\n",
    "        \n",
    "        \n",
    "        total_train_loss += loss.item()\n",
    "        num_train_batches += 1\n",
    "        # Log training loss every 100 mini-batches\n",
    "        if num_train_batches % 100 == 99:\n",
    "            avg_train_loss = total_train_loss / num_train_batches  # Calculate average up to the current batch\n",
    "            writer.add_scalar(\"Training Loss\", avg_train_loss, global_step)\n",
    "            \n",
    "            # Step Scheduler\n",
    "            scheduler.step(avg_train_loss)\n",
    "            \n",
    "            total_train_loss = 0.0\n",
    "            num_train_batches = 0\n",
    "        \n",
    "        global_step += 1\n",
    "    \n",
    "    # Deallocate GPU memory\n",
    "    target_nodes = None\n",
    "    target_edges = None\n",
    "    node_params_mask = None\n",
    "    pred_nodes = None\n",
    "    pred_edges = None\n",
    "    means = None\n",
    "    logvars = None\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # Validate Model at end of epoch\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        total_validate_loss = 0.0\n",
    "        num_validate_batches = 0\n",
    "        for batch_idx, (nodes, edges) in enumerate(tqdm(validate_loader)):\n",
    "            target_nodes = nodes.to(device)\n",
    "            target_edges = edges.to(device)\n",
    "\n",
    "            pred_nodes, pred_edges, means, logvars = model(target_nodes, target_edges)\n",
    "\n",
    "            loss = reconstruction_loss(pred_nodes, pred_edges, target_nodes, target_edges) \n",
    "            loss += 0.1*kl_loss(means, logvars)\n",
    "            \n",
    "            if (not loss.isfinite().all()):\n",
    "                raise ValueError(\"Loss is not finite\")\n",
    "        \n",
    "            # Save an 4 image pairs of an input and output of model\n",
    "            if num_validate_batches == 0:\n",
    "                fig, axes = plt.subplots(nrows = 4, ncols = 2, figsize=(8, 16))\n",
    "                fig.suptitle(f\"Target (left) vs Preds (right) for epoch {epoch}\")\n",
    "                for i in range(4):\n",
    "                    target_sketch = SketchDataset.preds_to_sketch(target_nodes[i].cpu(), target_edges[i].cpu())\n",
    "                    pred_sketch = SketchDataset.preds_to_sketch(pred_nodes[i].detach().cpu(), pred_edges[i].detach().cpu())\n",
    "                \n",
    "                    datalib.render_sketch(target_sketch, axes[i, 0])\n",
    "                    datalib.render_sketch(pred_sketch, axes[i, 1])\n",
    "            \n",
    "                writer.add_figure(f\"Epoch result visualization\", fig, epoch)\n",
    "                plt.close()\n",
    "            \n",
    "            \n",
    "            total_validate_loss += loss.item()\n",
    "            num_validate_batches += 1\n",
    "    \n",
    "        # Calculate and log the average validation loss for this epoch\n",
    "        avg_validate_loss = total_validate_loss / num_validate_batches  # Calculate average up to the last batch\n",
    "        writer.add_scalar(\"Validation Loss\", avg_validate_loss, epoch)\n",
    "\n",
    "        # Save the model checkpoint if the validation loss improves\n",
    "        if avg_validate_loss < best_validation_loss:\n",
    "            best_validation_loss = avg_validate_loss\n",
    "            checkpoint_path = f\"best_model_checkpoint.pth\"\n",
    "            torch.save(model.state_dict(), checkpoint_path)\n",
    "            print(f\"Saved model checkpoint with validation loss: {best_validation_loss:.4f} to {checkpoint_path}\")\n",
    "    \n",
    "    # Deallocate GPU memory\n",
    "    target_nodes = None\n",
    "    target_edges = None\n",
    "    node_params_mask = None\n",
    "    pred_nodes = None\n",
    "    pred_edges = None\n",
    "    means = None\n",
    "    logvars = None\n",
    "    torch.cuda.synchronize()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Test Model\n",
    "print(f\"Testing model ---\")\n",
    "model.eval()\n",
    "total_test_loss = 0.0\n",
    "num_test_batches = 0\n",
    "for batch_idx, (nodes, edges, node_params_mask) in enumerate(test_loader):\n",
    "    target_nodes = nodes.to(device)\n",
    "    target_edges = edges.to(device)\n",
    "    node_params_mask = node_params_mask.to(device)\n",
    "\n",
    "    pred_nodes, pred_edges, means, logvars = model(target_nodes, target_edges)\n",
    "    \n",
    "    loss = reconstruction_loss(pred_nodes, pred_edges, target_nodes, target_edges, node_params_mask) + kl_loss(means, logvars)\n",
    "\n",
    "    total_test_loss += loss.item()\n",
    "    num_test_batches += 1\n",
    "\n",
    "writer.add_scalar(\"Test Loss\", total_test_loss / num_test_batches)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from config import NUM_PRIMITIVE_TYPES, NUM_CONSTRAINT_TYPES\n",
    "\n",
    "def reconstruction_loss(pred_nodes : Tensor, pred_edges : Tensor, target_nodes : Tensor, target_edges : Tensor, node_params_mask : Tensor):\n",
    "    '''Node Loss'''\n",
    "    bce = F.binary_cross_entropy(input = pred_nodes[:,:,0], target = target_nodes[:,:,0], reduction = 'sum')\n",
    "\n",
    "    weight = torch.tensor([1.0, 4.0, 4.0, 3.0, 1.0]).to(pred_nodes.device)             # Weight circles, arcs, and points higher since they are much rarer than line and none types\n",
    "    primitive_type_labels = torch.argmax(target_nodes[:,:,1:6], dim = 2)               # batch_size x num_nodes (class index for each node)\n",
    "    primitive_type_logits = pred_nodes[:,:,1:6].permute(0,2,1).contiguous() # batch_size x num_primitive_types x num_nodes\n",
    "    \n",
    "    node_cross = F.nll_loss(input = primitive_type_logits, target = primitive_type_labels, weight = weight, reduction = 'sum')\n",
    "\n",
    "    # node_params_mask ensures that only relevant primtive parameters are used for loss \n",
    "    mse = F.mse_loss(input = pred_nodes[:,:,6:] * node_params_mask, target = target_nodes[:,:,6:], reduction='sum')\n",
    "\n",
    "    # Normalize losses to prevent mse loss from dominating\n",
    "    node_loss = bce + node_cross + mse\n",
    "    node_loss = bce + 4 * node_cross + 8 * mse\n",
    "    \n",
    "    '''Edge Loss'''\n",
    "    subnode_a_labels = torch.argmax(target_edges[:,:,:,0:4], dim = 3)\n",
    "    subnode_a_logits = pred_edges[:,:,:,0:4].permute(0, 3, 1, 2).contiguous()\n",
    "    sub_a_cross_entropy = F.nll_loss(input = subnode_a_logits, target = subnode_a_labels, reduction = 'sum')\n",
    "\n",
    "    subnode_b_labels = torch.argmax(target_edges[:,:,:,4:8], dim = 3)\n",
    "    subnode_b_logits = pred_edges[:,:,:,4:8].permute(0, 3, 1, 2).contiguous()\n",
    "    sub_b_cross_entropy = F.nll_loss(input = subnode_b_logits, target = subnode_b_labels, reduction = 'sum')\n",
    "\n",
    "    constraint_type_labels = torch.argmax(target_edges[:,:,:,8:], dim = 3)\n",
    "    constraint_type_logits = pred_edges[:,:,:,8:].permute(0, 3, 1, 2).contiguous()\n",
    "    constraint_cross_entropy = F.nll_loss(input = constraint_type_logits, target = constraint_type_labels, reduction = 'mean')\n",
    "\n",
    "    edge_loss = sub_a_cross_entropy + sub_b_cross_entropy + constraint_cross_entropy\n",
    "    edge_loss = sub_a_cross_entropy + sub_b_cross_entropy + constraint_cross_entropy\n",
    "    \n",
    "    return node_loss + 0.3 * edge_loss\n",
    "\n",
    "def kl_loss(means : Tensor, logvars : Tensor):\n",
    "    MAX_LOGVAR = 20\n",
    "    logvars = torch.clamp(input = logvars, max = MAX_LOGVAR)\n",
    "\n",
    "    kld = -0.5 * torch.sum(1 + logvars - means * means - torch.exp(logvars))\n",
    "    kld = torch.clamp(input = kld, max = 1000)\n",
    "    return kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import Tensor\n",
    "from config import NUM_PRIMITIVE_TYPES, NUM_CONSTRAINT_TYPES\n",
    "\n",
    "def reconstruction_loss(pred_nodes : Tensor, pred_edges : Tensor, target_nodes : Tensor, target_edges : Tensor, node_params_mask : Tensor):\n",
    "    '''Node Loss'''\n",
    "    weight = torch.tensor([1.0, 4.0, 4.0, 3.0, 0.1]).to(pred_nodes.device)  # Weight circles, arcs, and points higher since they are much rarer than line and none types\n",
    "    primitive_type_labels = torch.argmax(target_nodes[:,:,1:6], dim = 2)    # batch_size x num_nodes (class index for each node)\n",
    "    primitive_type_logits = pred_nodes[:,:,1:6].permute(0,2,1).contiguous() # batch_size x num_primitive_types x num_nodes\n",
    "    \n",
    "    node_cross = F.nll_loss(\n",
    "        input = primitive_type_logits.log(), \n",
    "        target = primitive_type_labels, \n",
    "        weight = weight, \n",
    "        reduction = 'sum')\n",
    "\n",
    "    # Only apply bce for primitives that are not none types\n",
    "    bce = F.binary_cross_entropy(\n",
    "        input = pred_nodes[primitive_type_labels != 4][:,0], \n",
    "        target = target_nodes[primitive_type_labels != 4][:,0],\n",
    "        reduction = 'sum')\n",
    "    # node_params_mask ensures that only relevant primtive parameters are used for loss \n",
    "    mse = F.mse_loss(input = pred_nodes[:,:,6:] * node_params_mask, target = target_nodes[:,:,6:], reduction='sum')\n",
    "\n",
    "    # Total node loss\n",
    "    node_loss = bce + 4 * node_cross + 8 * mse\n",
    "    \n",
    "    '''Edge Loss'''\n",
    "    constraint_type_labels = torch.argmax(target_edges[:,:,:,8:], dim = 3)\n",
    "    constraint_type_logits = pred_edges[:,:,:,8:].permute(0, 3, 1, 2).contiguous()\n",
    "    # There are far more none constraint types, so weigh them less\n",
    "    constraint_cross_entropy = F.nll_loss(\n",
    "        input = constraint_type_logits.log(), \n",
    "        target = constraint_type_labels,\n",
    "        weight = torch.tensor([1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.05]).to(pred_edges.device),\n",
    "        reduction = 'sum')\n",
    "    \n",
    "    # Only apply subnode loss to constraints that are not none -------\n",
    "    subnode_a_labels = torch.argmax(target_edges[:,:,:,0:4], dim = 3)[constraint_type_labels != 8]\n",
    "    subnode_a_logits = pred_edges[:,:,:,0:4][constraint_type_labels != 8]\n",
    "    sub_a_cross_entropy = F.nll_loss(\n",
    "        input = subnode_a_logits.log(), \n",
    "        target = subnode_a_labels, \n",
    "        reduction = 'sum')\n",
    "\n",
    "    subnode_b_labels = torch.argmax(target_edges[:,:,:,4:8], dim = 3)[constraint_type_labels != 8]\n",
    "    subnode_b_logits = pred_edges[:,:,:,4:8][constraint_type_labels != 8]\n",
    "    sub_b_cross_entropy = F.nll_loss(\n",
    "        input = subnode_b_logits.log(), \n",
    "        target = subnode_b_labels, \n",
    "        reduction = 'sum')\n",
    "\n",
    "\n",
    "    edge_loss = sub_a_cross_entropy + sub_b_cross_entropy + constraint_cross_entropy\n",
    "    \n",
    "    return node_loss + 0.3 * edge_loss\n",
    "\n",
    "def kl_loss(means : Tensor, logvars : Tensor):\n",
    "    # MAX_LOGVAR = 20\n",
    "    # logvars = torch.clamp(input = logvars, max = MAX_LOGVAR)\n",
    "\n",
    "    kld = -0.5 * torch.sum(1 + logvars - means * means - torch.exp(logvars))\n",
    "    # kld = torch.clamp(input = kld, max = 1000)\n",
    "    return kld"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ldr = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes, edges, node_params_mask = next(train_ldr)\n",
    "nodes2 = nodes.clone()\n",
    "nodes2[0,0,0] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(100.)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstruction_loss(nodes2, edges, nodes, edges, node_params_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
